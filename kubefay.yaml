---
# Source: kubefay/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubefay-agent
  namespace: kube-system
---
# Source: kubefay/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubefay-config
  namespace: kube-system
data:
  kubefay-agent.conf: |
    # FeatureGates is a map of feature names to bools that enable or disable experimental features.
    featureGates:
    # Enable AntreaProxy which provides ServiceLB for in-cluster Services in antrea-agent.
    # It should be enabled on Windows, otherwise NetworkPolicy will not take effect on
    # Service traffic.
    #  AntreaProxy: true
  
    # Enable traceflow which provides packet tracing feature to diagnose network issue.
    #  Traceflow: true
  
    # Enable Antrea ClusterNetworkPolicy feature to complement K8s NetworkPolicy for cluster admins
    # to define security policies which apply to the entire cluster, and Antrea NetworkPolicy
    # feature that supports priorities, rule actions and externalEntities in the future.
    #  AntreaPolicy: false
  
    # Enable flowexporter which exports polled conntrack connections as IPFIX flow records from each
    # agent to a configured collector.
    #  FlowExporter: false
  
    # Enable collecting and exposing NetworkPolicy statistics.
    #  NetworkPolicyStats: false
  
    # The feature gate for Antrea IPAM.
    #  AntreaIPAM: true
  
    # Name of the OpenVSwitch bridge antrea-agent will create and use.
    # Make sure it doesn't conflict with your existing OpenVSwitch bridges.
    #ovsBridge: br-int
  
    # Datapath type to use for the OpenVSwitch bridge created by Antrea. Supported values are:
    # - system
    # - netdev
    # 'system' is the default value and corresponds to the kernel datapath. Use 'netdev' to run
    # OVS in userspace mode. Userspace mode requires the tun device driver to be available.
    #ovsDatapathType: system
  
    # Name of the interface antrea-agent will create and use for host <--> pod communication.
    # Make sure it doesn't conflict with your existing interfaces.
    #hostGateway: antrea-gw0
  
    # Determines how traffic is encapsulated. It has the following options:
    # encap(default):    Inter-node Pod traffic is always encapsulated and Pod to external network
    #                    traffic is SNAT'd.
    # noEncap:           Inter-node Pod traffic is not encapsulated; Pod to external network traffic is
    #                    SNAT'd if noSNAT is not set to true. Underlying network must be capable of
    #                    supporting Pod traffic across IP subnets.
    # hybrid:            noEncap if source and destination Nodes are on the same subnet, otherwise encap.
    # networkPolicyOnly: Antrea enforces NetworkPolicy only, and utilizes CNI chaining and delegates Pod
    #                    IPAM and connectivity to the primary CNI.
    #
    #trafficEncapMode: encap
  
    # Whether or not to SNAT (using the Node IP) the egress traffic from a Pod to the external network.
    # This option is for the noEncap traffic mode only, and the default value is false. In the noEncap
    # mode, if the cluster's Pod CIDR is reachable from the external network, then the Pod traffic to
    # the external network needs not be SNAT'd. In the networkPolicyOnly mode, antrea-agent never
    # performs SNAT and this option will be ignored; for other modes it must be set to false.
    #noSNAT: false
  
    # Tunnel protocols used for encapsulating traffic across Nodes. Supported values:
    # - geneve (default)
    # - vxlan
    # - gre
    # - stt
    #tunnelType: geneve
  
    # Default MTU to use for the host gateway interface and the network interface of each Pod.
    # If omitted, antrea-agent will discover the MTU of the Node's primary interface and
    # also adjust MTU to accommodate for tunnel encapsulation overhead (if applicable).
    #defaultMTU: 1450
  
    # Whether or not to enable IPsec encryption of tunnel traffic. IPsec encryption is only supported
    # for the GRE tunnel type.
    #enableIPSecTunnel: false
  
    # ClusterIP CIDR range for Services. It's required when AntreaProxy is not enabled, and should be
    # set to the same value as the one specified by --service-cluster-ip-range for kube-apiserver. When
    # AntreaProxy is enabled, this parameter is not needed and will be ignored if provided.
    #serviceCIDR: 10.96.0.0/12
  
    # ClusterIP CIDR range for IPv6 Services. It's required when using kube-proxy to provide IPv6 Service in a Dual-Stack
    # cluster or an IPv6 only cluster. The value should be the same as the configuration for kube-apiserver specified by
    # --service-cluster-ip-range. When AntreaProxy is enabled, this parameter is not needed.
    # No default value for this field.
    #serviceCIDRv6:
  
    # The port for the antrea-agent APIServer to serve on.
    # Note that if it's set to another value, the `containerPort` of the `api` port of the
    # `antrea-agent` container must be set to the same value.
    #apiPort: 10350
  
    # Enable metrics exposure via Prometheus. Initializes Prometheus metrics listener.
    #enablePrometheusMetrics: true
  
    # Provide flow collector address as string with format <IP>:<port>[:<proto>], where proto is tcp or udp.
    # IP can be either IPv4 or IPv6. However, IPv6 address should be wrapped with [].
    # This also enables the flow exporter that sends IPFIX flow records of conntrack flows on OVS bridge.
    # If no L4 transport proto is given, we consider tcp as default.
    #flowCollectorAddr: ""
  
    # Provide flow poll interval as a duration string. This determines how often the flow exporter dumps connections from the conntrack module.
    # Flow poll interval should be greater than or equal to 1s (one second).
    # Valid time units are "ns", "us" (or "Âµs"), "ms", "s", "m", "h".
    #flowPollInterval: "5s"
  
    # Provide flow export frequency, which is the number of poll cycles elapsed before flow exporter exports flow records to
    # the flow collector.
    # Flow export frequency should be greater than or equal to 1.
    #flowExportFrequency: 12
  kubefay-cni.conflist: |
    {
        "cniVersion":"0.3.0",
        "name": "kubefay",
        "plugins": [
            {
                "type": "kubefay-cni",
                "ipam": {
                    "type": "kubefay-ipam-cni"
                }
            },
            {
                "type": "portmap",
                "capabilities": {"portMappings": true}
            },
            {
                "type": "bandwidth",
                "capabilities": {"bandwidth": true}
            }
        ]
    }
---
# Source: kubefay/templates/crds.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: subnets.kubefay.kubefay.github.com
spec:
  group: kubefay.kubefay.github.com
  names:
    kind: SubNet
    listKind: SubNetList
    plural: subnets
    singular: subnet
    shortNames:
    - sn
  scope: Namespaced
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                cidr:
                  type: string
                dns:
                  type: string
                externalIPs:
                  type: array
                  items:
                    type: string
                gateway:
                  type: string
                ipVersion:
                  type: string
                lastReservedIP:
                  type: string
                namespaces:
                  type: array
                  items:
                    type: string
                unusedPool:
                  type: array
                  items:
                    type: string
                usedPool:
                  type: object
                  additionalProperties:
                    type: string
            status:
                type: object
                properties:
                    poolStatus:
                      type: string
                    ipamEvent:
                      type: string
---
# Source: kubefay/templates/serviceaccount.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubefay-agent
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - pods
      - endpoints
      - services
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - watch
      - list
      - create
      - update
  - apiGroups:
      - clusterinformation.kubefay.tanzu.vmware.com
    resources:
      - kubefayagentinfos
    verbs:
      - get
      - create
      - update
      - delete
  - apiGroups:
      - controlplane.kubefay.tanzu.vmware.com
      - networking.kubefay.tanzu.vmware.com
    resources:
      - networkpolicies
      - appliedtogroups
      - addressgroups
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - controlplane.kubefay.tanzu.vmware.com
    resources:
      - nodestatssummaries
    verbs:
      - create
  - apiGroups:
      - controlplane.kubefay.tanzu.vmware.com
    resources:
      - networkpolicies/status
    verbs:
      - create
      - get
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
  # This is the content of built-in role kube-system/extension-apiserver-authentication-reader.
  # But it doesn't have list/watch permission before K8s v1.17.0 so the extension apiserver (kubefay-agent) will
  # have permission issue after bumping up apiserver library to a version that supports dynamic authentication.
  # See https://github.com/kubernetes/kubernetes/pull/85375
  # To support K8s clusters older than v1.17.0, we grant the required permissions directly instead of relying on
  # the extension-apiserver-authentication role.
  - apiGroups:
      - ""
    resourceNames:
      - extension-apiserver-authentication
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - kubefay-ca
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - ops.kubefay.tanzu.vmware.com
    resources:
      - traceflows
      - traceflows/status
    verbs:
      - get
      - watch
      - list
      - update
      - patch
      - create
      - delete
  - apiGroups:
      - kubefay.kubefay.github.com
    resources:
      - subnets
    verbs:
      - get
      - watch
      - list
      - update
      - patch
      - create
      - delete
---
# Source: kubefay/templates/serviceaccount.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubefay-agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubefay-agent
subjects:
  - kind: ServiceAccount
    name: kubefay-agent
    namespace: kube-system
---
# Source: kubefay/templates/agent.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kubefay-agent
  namespace: kube-system
  labels:
    component: kubefay-agent
spec:
  selector:
    matchLabels:
      component: kubefay-agent
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        component: kubefay-agent
    spec:
      hostNetwork: true
      priorityClassName: system-node-critical
      tolerations:
        # Mark it as a critical add-on.
        - key: CriticalAddonsOnly
          operator: Exists
        # Make sure it gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        # Make sure it doesn't get evicted.
        - effect: NoExecute
          operator: Exists
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: kubefay-agent
      initContainers:
        - name: install-cni
          image: "kubefay/kubefay-ubuntu:latest"
          imagePullPolicy: Never
          resources:
            requests:
              cpu: "100m"
          command: ["install_cni"]
          securityContext:
            capabilities:
              add:
                # SYS_MODULE is required to load the OVS kernel module.
                - SYS_MODULE
          volumeMounts:
          - name: kubefay-config
            mountPath: /etc/kubefay/kubefay-cni.conflist
            subPath: kubefay-cni.conflist
            readOnly: true
          - name: host-cni-conf
            mountPath: /host/etc/cni/net.d
          - name: host-cni-bin
            mountPath: /host/opt/cni/bin
          # For loading the OVS kernel module.
          - name: host-lib-modules
            mountPath: /lib/modules
            readOnly: true
          # depmod is required by modprobe when the Node OS is different from
          # that of the Antrea Docker image.
          - name: host-depmod
            mountPath: /sbin/depmod
            readOnly: true
          - name: host-var-run-kubefay
            mountPath: /var/run/kubefay
      containers:
        - name: kubefay-agent
          image: "kubefay/kubefay-ubuntu:latest"
          imagePullPolicy: Never
          resources:
            requests:
              cpu: "200m"
          command: ["kubefay-agent"]
          # Log to both "/var/log/kubefay/" and stderr (so "kubectl logs" can work).
          # args: ["--config", "/etc/kubefay/kubefay-agent.conf", "--logtostderr=false", "--log_dir=/var/log/kubefay", "--alsologtostderr", "--log_file_max_size=100", "--log_file_max_num=4", "--v=0"]
          args: ["--v=5"]
          env:
            # Provide pod and node information for clusterinformation CRD.
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 10350
              name: api
              protocol: TCP
          securityContext:
            # kubefay-agent needs to perform sysctl configuration.
            privileged: true
          volumeMounts:
          - name: kubefay-config
            mountPath: /etc/kubefay/kubefay-agent.conf
            subPath: kubefay-agent.conf
            readOnly: true
          - name: host-var-run-kubefay
            mountPath: /var/run/kubefay
          - name: host-var-run-kubefay
            mountPath: /var/run/openvswitch
            subPath: openvswitch
          # host-local IPAM stores allocated IP addresses as files in /var/lib/cni/networks/$NETWORK_NAME.
          # Mount a sub-directory of host-var-run-kubefay to it for persistence of IP allocation.
          - name: host-var-run-kubefay
            mountPath: /var/lib/cni
            subPath: cni
          # We need to mount both the /proc directory and the /var/run/netns directory so that
          # kubefay-agent can open the network namespace path when setting up Pod
          # networking. Different container runtimes may use /proc or /var/run/netns when invoking
          # the CNI commands. Docker uses /proc and containerd uses /var/run/netns.
          - name: host-var-log-kubefay
            mountPath: /var/log/kubefay
          - name: host-proc
            mountPath: /host/proc
            readOnly: true
          - name: host-var-run-netns
            mountPath: /host/var/run/netns
            readOnly: true
            # When a container is created, a mount point for the network namespace is added under
            # /var/run/netns on the host, which needs to be propagated to the kubefay-agent container.
            mountPropagation: HostToContainer
          - name: xtables-lock
            mountPath: /run/xtables.lock
        - name: kubefay-ovs
          image: "kubefay/kubefay-ubuntu:latest"
          imagePullPolicy: Never
          resources:
            requests:
              cpu: "200m"
          command: ["start_ovs"]
          args: ["--log_file_max_size=100", "--log_file_max_num=4"]
          securityContext:
            # capabilities required by OVS daemons
            capabilities:
              add:
                - SYS_NICE
                - NET_ADMIN
                - SYS_ADMIN
                - IPC_LOCK
          volumeMounts:
          - name: host-var-run-kubefay
            mountPath: /var/run/openvswitch
            subPath: openvswitch
          - name: host-var-log-kubefay
            mountPath: /var/log/openvswitch
            subPath: openvswitch
      volumes:
        - name: kubefay-config
          configMap:
            name: kubefay-config
        - name: host-cni-conf
          hostPath:
            path: /etc/cni/net.d
        - name: host-cni-bin
          hostPath:
            path: /opt/cni/bin
        - name: host-proc
          hostPath:
            path: /proc
        - name: host-var-run-netns
          hostPath:
            path: /var/run/netns
        - name: host-var-run-kubefay
          hostPath:
            path: /var/run/kubefay
            # we use subPath to create run subdirectories for different component (e.g. OVS) and
            # subPath requires the base volume to exist
            type: DirectoryOrCreate
        - name: host-var-log-kubefay
          hostPath:
            path: /var/log/kubefay
            # we use subPath to create logging subdirectories for different component (e.g. OVS)
            type: DirectoryOrCreate
        - name: host-lib-modules
          hostPath:
            path: /lib/modules
        - name: host-depmod
          hostPath:
            path: /sbin/depmod
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
