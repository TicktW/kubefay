---
# Source: kubefay/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubefay-agent
  namespace: kube-system
---
# Source: kubefay/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubefay-config
  namespace: kube-system
data:
  kubefay-agent.conf: ""
  kubefay-cni.conflist: |
    {
        "cniVersion":"0.3.0",
        "name": "kubefay",
        "plugins": [
            {
                "type": "kubefay-cni",
                "ipam": {
                    "type": "kubefay-ipam-cni"
                }
            },
            {
                "type": "portmap",
                "capabilities": {"portMappings": true}
            },
            {
                "type": "bandwidth",
                "capabilities": {"bandwidth": true}
            }
        ]
    }
---
# Source: kubefay/templates/crds.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: subnets.kubefay.kubefay.github.com
spec:
  group: kubefay.kubefay.github.com
  names:
    kind: SubNet
    listKind: SubNetList
    plural: subnets
    singular: subnet
    shortNames:
    - sn
  scope: Namespaced
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                cidr:
                  type: string
                dns:
                  type: string
                externalIPs:
                  type: array
                  items:
                    type: string
                gateway:
                  type: string
                ipVersion:
                  type: string
                lastReservedIP:
                  type: string
                namespaces:
                  type: array
                  items:
                    type: string
                unusedPool:
                  type: array
                  items:
                    type: string
                usedPool:
                  type: object
                  additionalProperties:
                    type: string
            status:
                type: object
                properties:
                    poolStatus:
                      type: string
                    ipamEvent:
                      type: string
---
# Source: kubefay/templates/serviceaccount.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubefay-agent
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - pods
      - endpoints
      - services
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - watch
      - list
      - create
      - update
  - apiGroups:
      - clusterinformation.kubefay.tanzu.vmware.com
    resources:
      - kubefayagentinfos
    verbs:
      - get
      - create
      - update
      - delete
  - apiGroups:
      - controlplane.kubefay.tanzu.vmware.com
      - networking.kubefay.tanzu.vmware.com
    resources:
      - networkpolicies
      - appliedtogroups
      - addressgroups
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - controlplane.kubefay.tanzu.vmware.com
    resources:
      - nodestatssummaries
    verbs:
      - create
  - apiGroups:
      - controlplane.kubefay.tanzu.vmware.com
    resources:
      - networkpolicies/status
    verbs:
      - create
      - get
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
  # This is the content of built-in role kube-system/extension-apiserver-authentication-reader.
  # But it doesn't have list/watch permission before K8s v1.17.0 so the extension apiserver (kubefay-agent) will
  # have permission issue after bumping up apiserver library to a version that supports dynamic authentication.
  # See https://github.com/kubernetes/kubernetes/pull/85375
  # To support K8s clusters older than v1.17.0, we grant the required permissions directly instead of relying on
  # the extension-apiserver-authentication role.
  - apiGroups:
      - ""
    resourceNames:
      - extension-apiserver-authentication
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - kubefay-ca
    verbs:
      - get
      - watch
      - list
  - apiGroups:
      - ops.kubefay.tanzu.vmware.com
    resources:
      - traceflows
      - traceflows/status
    verbs:
      - get
      - watch
      - list
      - update
      - patch
      - create
      - delete
  - apiGroups:
      - kubefay.kubefay.github.com
    resources:
      - subnets
    verbs:
      - get
      - watch
      - list
      - update
      - patch
      - create
      - delete
---
# Source: kubefay/templates/serviceaccount.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubefay-agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubefay-agent
subjects:
  - kind: ServiceAccount
    name: kubefay-agent
    namespace: kube-system
---
# Source: kubefay/templates/agent.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kubefay-agent
  namespace: kube-system
  labels:
    component: kubefay-agent
spec:
  selector:
    matchLabels:
      component: kubefay-agent
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        component: kubefay-agent
    spec:
      hostNetwork: true
      priorityClassName: system-node-critical
      tolerations:
        # Mark it as a critical add-on.
        - key: CriticalAddonsOnly
          operator: Exists
        # Make sure it gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        # Make sure it doesn't get evicted.
        - effect: NoExecute
          operator: Exists
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: kubefay-agent
      initContainers:
        - name: install-cni
          image: "kubefay/kubefay-ubuntu:latest"
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: "100m"
          command: ["install_cni"]
          securityContext:
            capabilities:
              add:
                # SYS_MODULE is required to load the OVS kernel module.
                - SYS_MODULE
          volumeMounts:
          - name: kubefay-config
            mountPath: /etc/kubefay/kubefay-cni.conflist
            subPath: kubefay-cni.conflist
            readOnly: true
          - name: host-cni-conf
            mountPath: /host/etc/cni/net.d
          - name: host-cni-bin
            mountPath: /host/opt/cni/bin
          # For loading the OVS kernel module.
          - name: host-lib-modules
            mountPath: /lib/modules
            readOnly: true
          # depmod is required by modprobe when the Node OS is different from
          # that of the Antrea Docker image.
          - name: host-depmod
            mountPath: /sbin/depmod
            readOnly: true
          - name: host-var-run-kubefay
            mountPath: /var/run/kubefay
      containers:
        - name: kubefay-agent
          image: "kubefay/kubefay-ubuntu:latest"
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: "200m"
          command: ["kubefay-agent"]
          # Log to both "/var/log/kubefay/" and stderr (so "kubectl logs" can work).
          # args: ["--config", "/etc/kubefay/kubefay-agent.conf", "--logtostderr=false", "--log_dir=/var/log/kubefay", "--alsologtostderr", "--log_file_max_size=100", "--log_file_max_num=4", "--v=0"]
          env:
            # Provide pod and node information for clusterinformation CRD.
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 10350
              name: api
              protocol: TCP
          securityContext:
            # kubefay-agent needs to perform sysctl configuration.
            privileged: true
          volumeMounts:
          - name: kubefay-config
            mountPath: /etc/kubefay/kubefay-agent.conf
            subPath: kubefay-agent.conf
            readOnly: true
          - name: host-var-run-kubefay
            mountPath: /var/run/kubefay
          - name: host-var-run-kubefay
            mountPath: /var/run/openvswitch
            subPath: openvswitch
          # host-local IPAM stores allocated IP addresses as files in /var/lib/cni/networks/$NETWORK_NAME.
          # Mount a sub-directory of host-var-run-kubefay to it for persistence of IP allocation.
          - name: host-var-run-kubefay
            mountPath: /var/lib/cni
            subPath: cni
          # We need to mount both the /proc directory and the /var/run/netns directory so that
          # kubefay-agent can open the network namespace path when setting up Pod
          # networking. Different container runtimes may use /proc or /var/run/netns when invoking
          # the CNI commands. Docker uses /proc and containerd uses /var/run/netns.
          - name: host-var-log-kubefay
            mountPath: /var/log/kubefay
          - name: host-proc
            mountPath: /host/proc
            readOnly: true
          - name: host-var-run-netns
            mountPath: /host/var/run/netns
            readOnly: true
            # When a container is created, a mount point for the network namespace is added under
            # /var/run/netns on the host, which needs to be propagated to the kubefay-agent container.
            mountPropagation: HostToContainer
          - name: xtables-lock
            mountPath: /run/xtables.lock
        - name: kubefay-ovs
          image: "kubefay/kubefay-ubuntu:latest"
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: "200m"
          command: ["start_ovs"]
          args: ["--log_file_max_size=100", "--log_file_max_num=4"]
          securityContext:
            # capabilities required by OVS daemons
            capabilities:
              add:
                - SYS_NICE
                - NET_ADMIN
                - SYS_ADMIN
                - IPC_LOCK
          volumeMounts:
          - name: host-var-run-kubefay
            mountPath: /var/run/openvswitch
            subPath: openvswitch
          - name: host-var-log-kubefay
            mountPath: /var/log/openvswitch
            subPath: openvswitch
      volumes:
        - name: kubefay-config
          configMap:
            name: kubefay-config
        - name: host-cni-conf
          hostPath:
            path: /etc/cni/net.d
        - name: host-cni-bin
          hostPath:
            path: /opt/cni/bin
        - name: host-proc
          hostPath:
            path: /proc
        - name: host-var-run-netns
          hostPath:
            path: /var/run/netns
        - name: host-var-run-kubefay
          hostPath:
            path: /var/run/kubefay
            # we use subPath to create run subdirectories for different component (e.g. OVS) and
            # subPath requires the base volume to exist
            type: DirectoryOrCreate
        - name: host-var-log-kubefay
          hostPath:
            path: /var/log/kubefay
            # we use subPath to create logging subdirectories for different component (e.g. OVS)
            type: DirectoryOrCreate
        - name: host-lib-modules
          hostPath:
            path: /lib/modules
        - name: host-depmod
          hostPath:
            path: /sbin/depmod
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
